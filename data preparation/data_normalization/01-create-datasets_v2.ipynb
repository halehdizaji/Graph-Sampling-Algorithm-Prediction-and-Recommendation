{"cells":[{"cell_type":"code","execution_count":1,"id":"69b9792c-3e27-4e68-8198-3980e9f1c919","metadata":{"executionInfo":{"elapsed":2604,"status":"ok","timestamp":1716958008050,"user":{"displayName":"Haleh Dizaji","userId":"03252501693667434351"},"user_tz":-120},"id":"69b9792c-3e27-4e68-8198-3980e9f1c919"},"outputs":[],"source":["import pandas as pd\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import make_regression\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"id":"9b4a8a15-f369-4237-b518-5101159dcfb4","metadata":{"id":"9b4a8a15-f369-4237-b518-5101159dcfb4"},"outputs":[],"source":["# zakaj potrebujem ponovno celotni pipeline?\n","# - smo normalizirali target values? ker se lahko drugače nauči za 0.1 in 0.3\n","# - smo ločili 0.1 in 0.3? Ker če ima skoraj vse iste vhodne podatke, je smiselno, da se loči?\n","#"]},{"cell_type":"code","execution_count":3,"id":"e72e519b-0bc3-4eba-8cb8-6a7d91bfa777","metadata":{"id":"e72e519b-0bc3-4eba-8cb8-6a7d91bfa777","executionInfo":{"status":"ok","timestamp":1716958041169,"user_tz":-120,"elapsed":279,"user":{"displayName":"Haleh Dizaji","userId":"03252501693667434351"}}},"outputs":[],"source":["root_folder = 'drive/MyDrive/Research/Projects/Graph_Sampling_Prediction/notebooks-export/'\n","sources = {'train': root_folder + 'data/generated_graphs/aggr_data/samplings/set_1/set_1_3_6_8_9_10_with_features.csv',\n","           'test_synth_medium': root_folder + 'data/generated_graphs/set_medium/all_graphs_sampling_results_with_features_v3.csv',\n","           'test_synth_large': root_folder + 'data/generated_graphs/set_large/all_graphs_sampling_results_with_features_v3.csv',\n","           'test_world_medium': root_folder +'data/real_graphs/set_medium/all_graphs_sampling_results_with_features.csv',\n","           'test_world_large': root_folder +'data/real_graphs/set_large/all_graphs_sampling_results_with_features_v3.csv'\n","           }"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DkG5RlY33syV","executionInfo":{"status":"ok","timestamp":1716958033352,"user_tz":-120,"elapsed":23430,"user":{"displayName":"Haleh Dizaji","userId":"03252501693667434351"}},"outputId":"decaa2b3-bfaa-4726-b16c-8605152679d7"},"id":"DkG5RlY33syV","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"id":"ee611451-3231-47c4-9803-e97a0f1012f7","metadata":{"id":"ee611451-3231-47c4-9803-e97a0f1012f7"},"outputs":[],"source":["#df[['Real value']].hist(bins=20)"]},{"cell_type":"code","execution_count":4,"id":"98fd58b3-15c4-4ce3-a206-097f0381867a","metadata":{"id":"98fd58b3-15c4-4ce3-a206-097f0381867a","executionInfo":{"status":"ok","timestamp":1716958047040,"user_tz":-120,"elapsed":277,"user":{"displayName":"Haleh Dizaji","userId":"03252501693667434351"}}},"outputs":[],"source":["def generate_dataset(file_paths, metric):\n","    train = pd.read_csv(file_paths['train'])\n","    test_synth_medium = pd.read_csv(file_paths['test_synth_medium'])\n","    test_synth_large = pd.read_csv(file_paths['test_synth_large'])\n","    test_world_medium = pd.read_csv(file_paths['test_world_medium'])\n","    test_world_large = pd.read_csv(file_paths['test_world_large'])\n","\n","    train['partition']='train'\n","    test_synth_medium['partition']='test'\n","    test_synth_large['partition']='test'\n","    test_world_medium['partition']='test'\n","    test_world_large['partition']='test'\n","\n","    train['synthetic']='synthetic'\n","    test_synth_medium['synthetic']='synthetic_medium'\n","    test_synth_large['synthetic']='synthetic_large'\n","    test_world_medium['synthetic']='realworld_medium'\n","    test_world_large['synthetic']='realworld_large'\n","\n","    #frames = [test_world]\n","    frames = [train, test_synth_medium, test_synth_large, test_world_medium, test_world_large]\n","    return pd.concat(frames)"]},{"cell_type":"code","execution_count":5,"id":"3ff72c98-82d9-4626-b765-2d462a1d4744","metadata":{"id":"3ff72c98-82d9-4626-b765-2d462a1d4744","executionInfo":{"status":"ok","timestamp":1716958052174,"user_tz":-120,"elapsed":274,"user":{"displayName":"Haleh Dizaji","userId":"03252501693667434351"}}},"outputs":[],"source":["def max_possible_edges_not_directed(n):\n","    return n*(n-1)/2\n","\n","# https://en.wikipedia.org/wiki/Betweenness_centrality\n","def scaling_factor_node_betweenness_centrality(n):\n","    return (n-1)*(n-2)/2 # undirected graphs\n","\n","def scaling_factor_edge_betweenness_centrality(n):\n","    return (n*(n-1))/2 # undirected graphs"]},{"cell_type":"code","execution_count":6,"id":"79417dd4-6772-4ee2-b96d-b2fd565eda6b","metadata":{"id":"79417dd4-6772-4ee2-b96d-b2fd565eda6b","executionInfo":{"status":"ok","timestamp":1716958054242,"user_tz":-120,"elapsed":285,"user":{"displayName":"Haleh Dizaji","userId":"03252501693667434351"}}},"outputs":[],"source":["def is_node_based(sampler_type):\n","    if(sampler_type in set(['random degree node', 'random node', 'random node edge'])):\n","        return 1\n","    return 0"]},{"cell_type":"code","execution_count":7,"id":"4b6d0ea2-5a23-4175-8110-6bdf339d94d6","metadata":{"id":"4b6d0ea2-5a23-4175-8110-6bdf339d94d6","executionInfo":{"status":"ok","timestamp":1716958055794,"user_tz":-120,"elapsed":289,"user":{"displayName":"Haleh Dizaji","userId":"03252501693667434351"}}},"outputs":[],"source":["def is_edge_based(sampler_type):\n","    if(sampler_type in set(['random edge', 'random node edge', 'induced random edge'])):\n","        return 1\n","    return 0"]},{"cell_type":"code","execution_count":8,"id":"6483b091-7223-427e-9184-c5d41f5ae09e","metadata":{"id":"6483b091-7223-427e-9184-c5d41f5ae09e","executionInfo":{"status":"ok","timestamp":1716958057477,"user_tz":-120,"elapsed":393,"user":{"displayName":"Haleh Dizaji","userId":"03252501693667434351"}}},"outputs":[],"source":["def is_traversal_based(sampler_type):\n","    if(sampler_type in set(['random jump', 'snowball', 'forest fire', 'metropolis hastings random walk', 'expansion', 'frontier', 'rank degree'])):\n","        return 1\n","    return 0"]},{"cell_type":"code","execution_count":9,"id":"3d0ef372-0a4f-4f94-adf0-cdd02ec8c7dd","metadata":{"id":"3d0ef372-0a4f-4f94-adf0-cdd02ec8c7dd","executionInfo":{"status":"ok","timestamp":1716958058753,"user_tz":-120,"elapsed":267,"user":{"displayName":"Haleh Dizaji","userId":"03252501693667434351"}}},"outputs":[],"source":["def add_sampler_type(df):\n","    df['sampler_type']=df.apply(lambda row: get_sampler_type(row['sampling algorithm']), axis=1)"]},{"cell_type":"code","execution_count":10,"id":"f438830d-8cc6-4a39-8bd3-de41b9bb6ec9","metadata":{"id":"f438830d-8cc6-4a39-8bd3-de41b9bb6ec9","executionInfo":{"status":"ok","timestamp":1716958078053,"user_tz":-120,"elapsed":275,"user":{"displayName":"Haleh Dizaji","userId":"03252501693667434351"}}},"outputs":[],"source":["def get_graph_param(graph_id):\n","    if '_Syn_' in graph_id:\n","        values = graph_id.split('range_size')[1].split('_param:')\n","        values = [values[0]] + values[1].split('_')\n","        return values[1]\n","    return ''"]},{"cell_type":"code","execution_count":11,"id":"8a6eca65-30bb-4961-a340-5734d0ebf9c7","metadata":{"id":"8a6eca65-30bb-4961-a340-5734d0ebf9c7","executionInfo":{"status":"ok","timestamp":1716958082828,"user_tz":-120,"elapsed":286,"user":{"displayName":"Haleh Dizaji","userId":"03252501693667434351"}}},"outputs":[],"source":["def generate_features(df):\n","    #df['graph param'] = df.apply(lambda row: get_graph_param(row['graph id']), axis=1)\n","\n","    mapping = {1:'snowball', 2:'random node', 3:'metropolis hastings random walk', 4: 'random degree node', 5: 'random jump', 6: 'random edge', 7: 'random node edge', 8: 'forest fire', 9:'expansion', 10: 'frontier', 11:'induced random edge', 12:'rank degree'}\n","\n","    #df['sampling_algorithm'] = df['sample_algs'].map(mapping)\n","\n","    df['node_count/edge_count']=np.exp(-np.log((df['node_nums']/df['edge_nums'])+1))\n","    df['edge_count/node_count']=np.exp(-np.log((df['edge_nums']/df['node_nums'])+1))\n","\n","    df['clust_coeff_max']=np.exp(-np.log(df['max_clust_coeff']+1))\n","    df['clust_coeff_min']=df['min_clust_coeff']/df['max_clust_coeff']\n","    df['clust_coeff_avg']=df['mean_clust_coeff']/df['max_clust_coeff']\n","    df['clust_coeff_var']=np.exp(-np.log(df['var_clust_coeff']+1))\n","    df['clust_coeff_median']=df['median_clust_coeff']/df['max_clust_coeff']\n","\n","    df['scaling_factor_node_betweenness_centrality']=df.apply(lambda row: max_possible_edges_not_directed(row['node_nums']), axis=1)\n","    #df['scaling_factor_edge_betweenness_centrality']=df.apply(lambda row: max_possible_edges_not_directed(row['node_nums']), axis=1)\n","\n","    df['degree_min']=df['min_degree']/df['max_degree']\n","    df['degree_avg']=df['mean_degree']/df['max_degree']\n","    df['degree_max']=np.exp(-np.log(df['max_degree']+1))\n","    # see: https://math.stackexchange.com/questions/2833062/a-measure-similar-to-variance-thats-always-between-0-and-1\n","    # we add log to alleviate how quickly the value approximates zero\n","    df['degree_var']=np.exp(-np.log(df['var_degree']+1))\n","    df['degree_median']=df['median_degree']/df['max_degree']\n","\n","    #'graph_density', 'min_clust_coeff', ''\n","    # 'mean_clust_coeff', 'var_clust_coeff', 'median_clust_coeff'\n","    df['node_betweenness_centrality_max']=np.exp(-(df['max_node_betweenness_centrality']/df['scaling_factor_node_betweenness_centrality']))\n","    df['node_betweenness_centrality_avg']=df['mean_node_betweenness_centrality']/df['scaling_factor_node_betweenness_centrality']\n","    df['node_betweenness_centrality_var']=np.exp(-np.log(df['var_node_betweenness_centrality']+1))\n","    df['node_betweenness_centrality_median']=df['median_node_betweenness_centrality']/df['scaling_factor_node_betweenness_centrality']\n","    df['node_betweenness_centrality_min']=df['min_node_betweenness_centrality']/df['scaling_factor_node_betweenness_centrality']\n","\n","    '''\n","    df['edge_betweenness_centrality_max']=np.exp(-(df['max_edge_betweenness_centrality']/df['scaling_factor_edge_betweenness_centrality']))\n","    df['edge_betweenness_centrality_avg']=df['mean_edge_betweenness_centrality']/df['scaling_factor_edge_betweenness_centrality']\n","    df['edge_betweenness_centrality_var']=np.exp(-np.log(df['var_edge_betweenness_centrality']+1))\n","    df['edge_betweenness_centrality_median']=df['median_edge_betweenness_centrality']/df['scaling_factor_edge_betweenness_centrality']\n","\n","    df['eccentricity_centrality_min']=df['min_eccentricity_centrality']/df['max_eccentricity_centrality']\n","    df['eccentricity_centrality_avg']=df['mean_eccentricity_centrality']/df['max_eccentricity_centrality']\n","    df['eccentricity_centrality_var']=np.exp(-np.log(df['var_eccentricity_centrality']+1))\n","    df['eccentricity_centrality_median']=df['median_eccentricity_centrality']/df['max_eccentricity_centrality']\n","    '''\n","\n","    df['eigenvector_centrality_min']=df['min_eigenvector_centrality']/df['max_eigenvector_centrality']\n","    df['eigenvector_centrality_avg']=df['mean_eigenvector_centrality']/df['max_eigenvector_centrality']\n","    df['eigenvector_centrality_median']=df['median_eigenvector_centrality']/df['max_eigenvector_centrality']\n","    df['eigenvector_centrality_var']=np.exp(-np.log(df['var_eigenvector_centrality']+1))\n","    df['eigenvector_centrality_max'] = np.exp(-np.log(df['max_eigenvector_centrality']+1))\n","\n","    df['degrees_spanning_tree_min']=df['min_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","    df['degrees_spanning_tree_avg']=df['mean_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","    df['degrees_spanning_tree_var']=df['var_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","    df['degrees_spanning_tree_median']=df['median_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","    df['degrees_spanning_tree_max']=np.exp(-np.log(df['max_degrees_max_spanning_tree']+1))\n","\n","    df['min_connected_components_size'] = df['min_connected_components_size']/df['max_connected_components_size']\n","    df['mean_connected_components_size'] = df['mean_connected_components_size']/df['max_connected_components_size']\n","    df['median_connected_components_size'] = df['median_connected_components_size']/df['max_connected_components_size']\n","    df['var_connected_components_size'] = np.exp(-np.log(df['var_connected_components_size']+1))\n","    df['num_connected_components'] = np.exp(-np.log(df['num_connected_components']+1))\n","    df['max_connected_components_size'] = np.exp(-np.log(df['max_connected_components_size']+1))\n","\n","    df['pagerank_centrality_min'] = df['min_pagerank_centrality']/df['max_pagerank_centrality']\n","    df['pagerank_centrality_avg'] = df['mean_pagerank_centrality']/df['max_pagerank_centrality']\n","    df['pagerank_centrality_median'] = df['median_pagerank_centrality']/df['max_pagerank_centrality']\n","    df['pagerank_centrality_var'] = np.exp(-np.log(df['var_pagerank_centrality']+1))\n","    df['pagerank_centrality_max']=np.exp(-np.log(df['max_pagerank_centrality']+1))\n","\n","    df['shortest_path_length_min'] = df['min_shortest_path_length']/df['max_shortest_path_length']\n","    df['shortest_path_length_avg'] = df['mean_shortest_path_length']/df['max_shortest_path_length']\n","    df['shortest_path_length_var'] = np.exp(-np.log(df['var_shortest_path_length']+1))\n","    df['shortest_path_length_max']=np.exp(-np.log(df['max_shortest_path_length']+1))\n","    '''\n","    df['shortest_path_length_lcc_min'] = df['min_shortest_path_length_LCC']/df['max_shortest_path_length_LCC']\n","    df['shortest_path_length_lcc_mean'] = df['mean_shortest_path_length_LCC']/df['max_shortest_path_length_LCC']\n","    df['shortest_path_length_lcc_var'] = np.exp(-np.log(df['var_shortest_path_length_LCC']+1))\n","    df['shortest_path_length_lcc_max']=np.exp(-np.log(df['max_shortest_path_length_LCC']+1))\n","    '''\n","    df['sampler_type_node_based']=df.apply(lambda row: is_node_based(row['sampling_algorithm']), axis=1)\n","    df['sampler_type_edge_based']=df.apply(lambda row: is_edge_based(row['sampling_algorithm']), axis=1)\n","    df['sampler_type_traversal_based']=df.apply(lambda row: is_traversal_based(row['sampling_algorithm']), axis=1)\n","\n","    # calc time features\n","    df['clust_coeff_calc_time'] = np.exp(-np.log(df['clust_coeff_calc_time']+1))\n","    df['connected_components_calc_time'] = np.exp(-np.log(df['connected_components_calc_time']+1))\n","    df['degree_assortativity_calc_time'] = np.exp(-np.log(df['degree_assortativity_calc_time']+1))\n","    df['eigenvector_centrality_calc_time'] = np.exp(-np.log(df['eigenvector_centrality_calc_time']+1))\n","    df['max_spanning_tree_calc_time'] = np.exp(-np.log(df['max_spanning_tree_calc_time']+1))\n","    df['pagerank_centrality_calc_time'] = np.exp(-np.log(df['pagerank_centrality_calc_time']+1))\n","\n","    # size features\n","    # model 1\n","    #df['node_nums'] = df['node_nums']/1000000\n","    #df['edge_nums'] = df['edge_nums']/100000000\n","\n","    # model 2\n","    df['node_nums'] = np.exp(-np.log(df['node_nums']+1))\n","    df['edge_nums'] = np.exp(-np.log(df['edge_nums']+1))\n","\n","    #print('df shape before join ', df.shape)\n","    #print('df index before join ', df.columns)\n","    #one_hot = pd.get_dummies(df['sampling_algorithm']).replace({False: 0, True: 1})\n","    #print('one_hot indx ', one_hot.index)\n","    #print('one_hot ', one_hot)\n","    df['sampling algorithm'] = df['sampling_algorithm']\n","    df = pd.get_dummies(df, columns=['sampling_algorithm'], prefix='', prefix_sep='').replace({False: 0, True: 1})\n","    return df\n","    #return df"]},{"cell_type":"code","execution_count":12,"id":"1dfa3126-180a-4d86-a020-6341999b78ef","metadata":{"id":"1dfa3126-180a-4d86-a020-6341999b78ef","executionInfo":{"status":"ok","timestamp":1716958088962,"user_tz":-120,"elapsed":249,"user":{"displayName":"Haleh Dizaji","userId":"03252501693667434351"}}},"outputs":[],"source":["def data_quality_check(df):\n","    nalist = df.columns[df.isna().any()].tolist()\n","    print('nalist ', nalist)\n","    if len(nalist)!=0:\n","        print(df[df['graph_ID'].isnull()])"]},{"cell_type":"code","execution_count":14,"id":"44aafc90-091b-41f0-83de-ce2215138e27","metadata":{"id":"44aafc90-091b-41f0-83de-ce2215138e27","executionInfo":{"status":"ok","timestamp":1716958148257,"user_tz":-120,"elapsed":278,"user":{"displayName":"Haleh Dizaji","userId":"03252501693667434351"}}},"outputs":[],"source":["def now_vs_after(df):\n","    print('Now: {}, after: {}'.format(len(df.index), len(df.drop_duplicates().index)))"]},{"cell_type":"code","execution_count":null,"id":"2fef567b-2ee5-4a45-9a88-9d71dfbca865","metadata":{"id":"2fef567b-2ee5-4a45-9a88-9d71dfbca865","outputId":"5e65d73c-9701-434d-d1d7-bb57c88a85a7"},"outputs":[{"ename":"NameError","evalue":"name 'pd' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#df = generate_dataset(sources, target)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m generate_features(\u001b[43mgenerate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset \u001b[39m\u001b[38;5;124m'\u001b[39m, dataset)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mcolumns)\n","Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mgenerate_dataset\u001b[0;34m(file_paths, metric)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_dataset\u001b[39m(file_paths, metric):\n\u001b[0;32m----> 2\u001b[0m     train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(file_paths[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#test_synth = pd.read_csv(file_paths['test_synth'])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#test_world = pd.read_csv(file_paths['test_world'])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartition\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}],"source":["target='D3'\n","#df = generate_dataset(sources, target)\n","dataset = generate_features(generate_dataset(sources, target)).drop_duplicates().fillna(0)\n","print('dataset ', dataset)\n","print(dataset.columns)"]},{"cell_type":"code","execution_count":null,"id":"2dfbc391-8493-441e-9425-581ba2278d1b","metadata":{"id":"2dfbc391-8493-441e-9425-581ba2278d1b"},"outputs":[],"source":["# quality check - ensure all values are between zero and one\n","for feature in ['node_count/edge_count', 'edge_count/node_count', 'clust_coeff_max', 'clust_coeff_min', 'clust_coeff_avg',\n","            'clust_coeff_var', 'clust_coeff_median', 'degree_min',\n","            'degree_avg', 'degree_var', 'degree_median', 'node_betweenness_centrality_max', 'node_betweenness_centrality_avg',\n","            'node_betweenness_centrality_var', 'node_betweenness_centrality_median',\n","            'edge_betweenness_centrality_max', 'edge_betweenness_centrality_avg',\n","            'edge_betweenness_centrality_var', 'edge_betweenness_centrality_median', 'eccentricity_centrality_min',\n","            'eccentricity_centrality_avg', 'eccentricity_centrality_var', 'eccentricity_centrality_median',\n","            'eigenvector_centrality_min', 'eigenvector_centrality_var', 'eigenvector_centrality_avg',\n","            'pagerank_centrality_var', 'degrees_spanning_tree_min', 'degrees_spanning_tree_avg', 'degrees_spanning_tree_var',\n","            'min_pagerank_centrality', 'max_pagerank_centrality', 'mean_pagerank_centrality', 'median_pagerank_centrality',\n","            'graph_density']:\n","    if dataset[feature].max()>1:\n","        print('Feature: {}, value: {}'.format(feature, dataset[feature].max()))"]},{"cell_type":"code","execution_count":15,"id":"a8112e73-0861-4eb9-977f-94e025572483","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22507,"status":"ok","timestamp":1716958175580,"user":{"displayName":"Haleh Dizaji","userId":"03252501693667434351"},"user_tz":-120},"id":"a8112e73-0861-4eb9-977f-94e025572483","outputId":"b58d154e-2de6-4c54-9340-c2872a6a98aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Target: D3\n","nalist  ['Unnamed: 0', 'entropy_clust_coeff', 'global_clust_coeff', 'global_clust_coeff_calc_time', 'node_betweenness_centrality_calc_time', 'min_shortest_path_length_LCC', 'max_shortest_path_length_LCC', 'var_shortest_path_length_LCC', 'mean_shortest_path_length_LCC', 'node_edge_betweenness_centrality_calc_time', 'min_edge_betweenness_centrality', 'max_edge_betweenness_centrality', 'mean_edge_betweenness_centrality', 'var_edge_betweenness_centrality', 'median_edge_betweenness_centrality', 'min_eccentricity_centrality', 'max_eccentricity_centrality', 'mean_eccentricity_centrality', 'median_eccentricity_centrality', 'var_eccentricity_centrality', 'entropy_degrees', 'diameter', 'diameter_calc_time', 'min_farness_centrality', 'max_farness_centrality', 'var_farness_centrality', 'mean_farness_centrality', 'median_farness_centrality', 'farness_centrality_calc_time', 'median_shortest_path_length_LCC', 'shortest_path_length_LCC_calc_time']\n","Empty DataFrame\n","Columns: [Unnamed: 0, graph_ID, min_clust_coeff, max_clust_coeff, mean_clust_coeff, var_clust_coeff, median_clust_coeff, clust_coeff_calc_time, min_node_betweenness_centrality, max_node_betweenness_centrality, mean_node_betweenness_centrality, var_node_betweenness_centrality, median_node_betweenness_centrality, num_connected_components, max_connected_components_size, min_connected_components_size, mean_connected_components_size, var_connected_components_size, median_connected_components_size, connected_components_calc_time, min_eigenvector_centrality, max_eigenvector_centrality, mean_eigenvector_centrality, var_eigenvector_centrality, median_eigenvector_centrality, eigenvector_centrality_calc_time, min_pagerank_centrality, max_pagerank_centrality, mean_pagerank_centrality, var_pagerank_centrality, median_pagerank_centrality, pagerank_centrality_calc_time, min_degrees_max_spanning_tree, max_degrees_max_spanning_tree, mean_degrees_max_spanning_tree, var_degrees_max_spanning_tree, median_degrees_max_spanning_tree, max_spanning_tree_calc_time, min_shortest_path_length, max_shortest_path_length, var_shortest_path_length, mean_shortest_path_length, degree_assortativity, degree_assortativity_calc_time, node_nums, edge_nums, min_degree, max_degree, mean_degree, var_degree, median_degree, graph_density, node_nums_div_edge_nums, edge_nums_div_node_nums, sampling_percent, sampling_algorithm, run_time, run_time var, D3, C2D2, HPD2, HPD2_LCC, KS Degree Distr var, KS Clustering Coefficient Distr var, KS hop plots Distr var, KS hop plots LCC Distr var, partition, synthetic, entropy_clust_coeff, global_clust_coeff, global_clust_coeff_calc_time, node_betweenness_centrality_calc_time, min_shortest_path_length_LCC, max_shortest_path_length_LCC, var_shortest_path_length_LCC, mean_shortest_path_length_LCC, node_edge_betweenness_centrality_calc_time, min_edge_betweenness_centrality, max_edge_betweenness_centrality, mean_edge_betweenness_centrality, var_edge_betweenness_centrality, median_edge_betweenness_centrality, min_eccentricity_centrality, max_eccentricity_centrality, mean_eccentricity_centrality, median_eccentricity_centrality, var_eccentricity_centrality, entropy_degrees, diameter, diameter_calc_time, min_farness_centrality, max_farness_centrality, var_farness_centrality, mean_farness_centrality, median_farness_centrality, farness_centrality_calc_time, median_shortest_path_length_LCC, shortest_path_length_LCC_calc_time]\n","Index: []\n","\n","[0 rows x 98 columns]\n","Now: 14143, after: 14143\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-11-ca77b9190fcd>:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['eigenvector_centrality_max'] = np.exp(-np.log(df['max_eigenvector_centrality']+1))\n","<ipython-input-11-ca77b9190fcd>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_min']=df['min_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_avg']=df['mean_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:56: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_var']=df['var_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_median']=df['median_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_max']=np.exp(-np.log(df['max_degrees_max_spanning_tree']+1))\n","<ipython-input-11-ca77b9190fcd>:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_min'] = df['min_pagerank_centrality']/df['max_pagerank_centrality']\n","<ipython-input-11-ca77b9190fcd>:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_avg'] = df['mean_pagerank_centrality']/df['max_pagerank_centrality']\n","<ipython-input-11-ca77b9190fcd>:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_median'] = df['median_pagerank_centrality']/df['max_pagerank_centrality']\n","<ipython-input-11-ca77b9190fcd>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_var'] = np.exp(-np.log(df['var_pagerank_centrality']+1))\n","<ipython-input-11-ca77b9190fcd>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_max']=np.exp(-np.log(df['max_pagerank_centrality']+1))\n","<ipython-input-11-ca77b9190fcd>:73: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_min'] = df['min_shortest_path_length']/df['max_shortest_path_length']\n","<ipython-input-11-ca77b9190fcd>:74: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_avg'] = df['mean_shortest_path_length']/df['max_shortest_path_length']\n","<ipython-input-11-ca77b9190fcd>:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_var'] = np.exp(-np.log(df['var_shortest_path_length']+1))\n","<ipython-input-11-ca77b9190fcd>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_max']=np.exp(-np.log(df['max_shortest_path_length']+1))\n","<ipython-input-11-ca77b9190fcd>:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampler_type_node_based']=df.apply(lambda row: is_node_based(row['sampling_algorithm']), axis=1)\n","<ipython-input-11-ca77b9190fcd>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampler_type_edge_based']=df.apply(lambda row: is_edge_based(row['sampling_algorithm']), axis=1)\n","<ipython-input-11-ca77b9190fcd>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampler_type_traversal_based']=df.apply(lambda row: is_traversal_based(row['sampling_algorithm']), axis=1)\n","<ipython-input-11-ca77b9190fcd>:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampling algorithm'] = df['sampling_algorithm']\n"]},{"output_type":"stream","name":"stdout","text":["Now: 14143, after: 14143\n","Target: C2D2\n","nalist  ['Unnamed: 0', 'entropy_clust_coeff', 'global_clust_coeff', 'global_clust_coeff_calc_time', 'node_betweenness_centrality_calc_time', 'min_shortest_path_length_LCC', 'max_shortest_path_length_LCC', 'var_shortest_path_length_LCC', 'mean_shortest_path_length_LCC', 'node_edge_betweenness_centrality_calc_time', 'min_edge_betweenness_centrality', 'max_edge_betweenness_centrality', 'mean_edge_betweenness_centrality', 'var_edge_betweenness_centrality', 'median_edge_betweenness_centrality', 'min_eccentricity_centrality', 'max_eccentricity_centrality', 'mean_eccentricity_centrality', 'median_eccentricity_centrality', 'var_eccentricity_centrality', 'entropy_degrees', 'diameter', 'diameter_calc_time', 'min_farness_centrality', 'max_farness_centrality', 'var_farness_centrality', 'mean_farness_centrality', 'median_farness_centrality', 'farness_centrality_calc_time', 'median_shortest_path_length_LCC', 'shortest_path_length_LCC_calc_time']\n","Empty DataFrame\n","Columns: [Unnamed: 0, graph_ID, min_clust_coeff, max_clust_coeff, mean_clust_coeff, var_clust_coeff, median_clust_coeff, clust_coeff_calc_time, min_node_betweenness_centrality, max_node_betweenness_centrality, mean_node_betweenness_centrality, var_node_betweenness_centrality, median_node_betweenness_centrality, num_connected_components, max_connected_components_size, min_connected_components_size, mean_connected_components_size, var_connected_components_size, median_connected_components_size, connected_components_calc_time, min_eigenvector_centrality, max_eigenvector_centrality, mean_eigenvector_centrality, var_eigenvector_centrality, median_eigenvector_centrality, eigenvector_centrality_calc_time, min_pagerank_centrality, max_pagerank_centrality, mean_pagerank_centrality, var_pagerank_centrality, median_pagerank_centrality, pagerank_centrality_calc_time, min_degrees_max_spanning_tree, max_degrees_max_spanning_tree, mean_degrees_max_spanning_tree, var_degrees_max_spanning_tree, median_degrees_max_spanning_tree, max_spanning_tree_calc_time, min_shortest_path_length, max_shortest_path_length, var_shortest_path_length, mean_shortest_path_length, degree_assortativity, degree_assortativity_calc_time, node_nums, edge_nums, min_degree, max_degree, mean_degree, var_degree, median_degree, graph_density, node_nums_div_edge_nums, edge_nums_div_node_nums, sampling_percent, sampling_algorithm, run_time, run_time var, D3, C2D2, HPD2, HPD2_LCC, KS Degree Distr var, KS Clustering Coefficient Distr var, KS hop plots Distr var, KS hop plots LCC Distr var, partition, synthetic, entropy_clust_coeff, global_clust_coeff, global_clust_coeff_calc_time, node_betweenness_centrality_calc_time, min_shortest_path_length_LCC, max_shortest_path_length_LCC, var_shortest_path_length_LCC, mean_shortest_path_length_LCC, node_edge_betweenness_centrality_calc_time, min_edge_betweenness_centrality, max_edge_betweenness_centrality, mean_edge_betweenness_centrality, var_edge_betweenness_centrality, median_edge_betweenness_centrality, min_eccentricity_centrality, max_eccentricity_centrality, mean_eccentricity_centrality, median_eccentricity_centrality, var_eccentricity_centrality, entropy_degrees, diameter, diameter_calc_time, min_farness_centrality, max_farness_centrality, var_farness_centrality, mean_farness_centrality, median_farness_centrality, farness_centrality_calc_time, median_shortest_path_length_LCC, shortest_path_length_LCC_calc_time]\n","Index: []\n","\n","[0 rows x 98 columns]\n","Now: 14143, after: 14143\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-11-ca77b9190fcd>:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['eigenvector_centrality_max'] = np.exp(-np.log(df['max_eigenvector_centrality']+1))\n","<ipython-input-11-ca77b9190fcd>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_min']=df['min_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_avg']=df['mean_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:56: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_var']=df['var_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_median']=df['median_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_max']=np.exp(-np.log(df['max_degrees_max_spanning_tree']+1))\n","<ipython-input-11-ca77b9190fcd>:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_min'] = df['min_pagerank_centrality']/df['max_pagerank_centrality']\n","<ipython-input-11-ca77b9190fcd>:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_avg'] = df['mean_pagerank_centrality']/df['max_pagerank_centrality']\n","<ipython-input-11-ca77b9190fcd>:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_median'] = df['median_pagerank_centrality']/df['max_pagerank_centrality']\n","<ipython-input-11-ca77b9190fcd>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_var'] = np.exp(-np.log(df['var_pagerank_centrality']+1))\n","<ipython-input-11-ca77b9190fcd>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_max']=np.exp(-np.log(df['max_pagerank_centrality']+1))\n","<ipython-input-11-ca77b9190fcd>:73: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_min'] = df['min_shortest_path_length']/df['max_shortest_path_length']\n","<ipython-input-11-ca77b9190fcd>:74: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_avg'] = df['mean_shortest_path_length']/df['max_shortest_path_length']\n","<ipython-input-11-ca77b9190fcd>:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_var'] = np.exp(-np.log(df['var_shortest_path_length']+1))\n","<ipython-input-11-ca77b9190fcd>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_max']=np.exp(-np.log(df['max_shortest_path_length']+1))\n","<ipython-input-11-ca77b9190fcd>:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampler_type_node_based']=df.apply(lambda row: is_node_based(row['sampling_algorithm']), axis=1)\n","<ipython-input-11-ca77b9190fcd>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampler_type_edge_based']=df.apply(lambda row: is_edge_based(row['sampling_algorithm']), axis=1)\n","<ipython-input-11-ca77b9190fcd>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampler_type_traversal_based']=df.apply(lambda row: is_traversal_based(row['sampling_algorithm']), axis=1)\n","<ipython-input-11-ca77b9190fcd>:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampling algorithm'] = df['sampling_algorithm']\n"]},{"output_type":"stream","name":"stdout","text":["Now: 14143, after: 14143\n","Target: HPD2\n","nalist  ['Unnamed: 0', 'entropy_clust_coeff', 'global_clust_coeff', 'global_clust_coeff_calc_time', 'node_betweenness_centrality_calc_time', 'min_shortest_path_length_LCC', 'max_shortest_path_length_LCC', 'var_shortest_path_length_LCC', 'mean_shortest_path_length_LCC', 'node_edge_betweenness_centrality_calc_time', 'min_edge_betweenness_centrality', 'max_edge_betweenness_centrality', 'mean_edge_betweenness_centrality', 'var_edge_betweenness_centrality', 'median_edge_betweenness_centrality', 'min_eccentricity_centrality', 'max_eccentricity_centrality', 'mean_eccentricity_centrality', 'median_eccentricity_centrality', 'var_eccentricity_centrality', 'entropy_degrees', 'diameter', 'diameter_calc_time', 'min_farness_centrality', 'max_farness_centrality', 'var_farness_centrality', 'mean_farness_centrality', 'median_farness_centrality', 'farness_centrality_calc_time', 'median_shortest_path_length_LCC', 'shortest_path_length_LCC_calc_time']\n","Empty DataFrame\n","Columns: [Unnamed: 0, graph_ID, min_clust_coeff, max_clust_coeff, mean_clust_coeff, var_clust_coeff, median_clust_coeff, clust_coeff_calc_time, min_node_betweenness_centrality, max_node_betweenness_centrality, mean_node_betweenness_centrality, var_node_betweenness_centrality, median_node_betweenness_centrality, num_connected_components, max_connected_components_size, min_connected_components_size, mean_connected_components_size, var_connected_components_size, median_connected_components_size, connected_components_calc_time, min_eigenvector_centrality, max_eigenvector_centrality, mean_eigenvector_centrality, var_eigenvector_centrality, median_eigenvector_centrality, eigenvector_centrality_calc_time, min_pagerank_centrality, max_pagerank_centrality, mean_pagerank_centrality, var_pagerank_centrality, median_pagerank_centrality, pagerank_centrality_calc_time, min_degrees_max_spanning_tree, max_degrees_max_spanning_tree, mean_degrees_max_spanning_tree, var_degrees_max_spanning_tree, median_degrees_max_spanning_tree, max_spanning_tree_calc_time, min_shortest_path_length, max_shortest_path_length, var_shortest_path_length, mean_shortest_path_length, degree_assortativity, degree_assortativity_calc_time, node_nums, edge_nums, min_degree, max_degree, mean_degree, var_degree, median_degree, graph_density, node_nums_div_edge_nums, edge_nums_div_node_nums, sampling_percent, sampling_algorithm, run_time, run_time var, D3, C2D2, HPD2, HPD2_LCC, KS Degree Distr var, KS Clustering Coefficient Distr var, KS hop plots Distr var, KS hop plots LCC Distr var, partition, synthetic, entropy_clust_coeff, global_clust_coeff, global_clust_coeff_calc_time, node_betweenness_centrality_calc_time, min_shortest_path_length_LCC, max_shortest_path_length_LCC, var_shortest_path_length_LCC, mean_shortest_path_length_LCC, node_edge_betweenness_centrality_calc_time, min_edge_betweenness_centrality, max_edge_betweenness_centrality, mean_edge_betweenness_centrality, var_edge_betweenness_centrality, median_edge_betweenness_centrality, min_eccentricity_centrality, max_eccentricity_centrality, mean_eccentricity_centrality, median_eccentricity_centrality, var_eccentricity_centrality, entropy_degrees, diameter, diameter_calc_time, min_farness_centrality, max_farness_centrality, var_farness_centrality, mean_farness_centrality, median_farness_centrality, farness_centrality_calc_time, median_shortest_path_length_LCC, shortest_path_length_LCC_calc_time]\n","Index: []\n","\n","[0 rows x 98 columns]\n","Now: 14143, after: 14143\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-11-ca77b9190fcd>:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['eigenvector_centrality_max'] = np.exp(-np.log(df['max_eigenvector_centrality']+1))\n","<ipython-input-11-ca77b9190fcd>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_min']=df['min_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_avg']=df['mean_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:56: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_var']=df['var_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_median']=df['median_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_max']=np.exp(-np.log(df['max_degrees_max_spanning_tree']+1))\n","<ipython-input-11-ca77b9190fcd>:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_min'] = df['min_pagerank_centrality']/df['max_pagerank_centrality']\n","<ipython-input-11-ca77b9190fcd>:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_avg'] = df['mean_pagerank_centrality']/df['max_pagerank_centrality']\n","<ipython-input-11-ca77b9190fcd>:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_median'] = df['median_pagerank_centrality']/df['max_pagerank_centrality']\n","<ipython-input-11-ca77b9190fcd>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_var'] = np.exp(-np.log(df['var_pagerank_centrality']+1))\n","<ipython-input-11-ca77b9190fcd>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_max']=np.exp(-np.log(df['max_pagerank_centrality']+1))\n","<ipython-input-11-ca77b9190fcd>:73: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_min'] = df['min_shortest_path_length']/df['max_shortest_path_length']\n","<ipython-input-11-ca77b9190fcd>:74: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_avg'] = df['mean_shortest_path_length']/df['max_shortest_path_length']\n","<ipython-input-11-ca77b9190fcd>:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_var'] = np.exp(-np.log(df['var_shortest_path_length']+1))\n","<ipython-input-11-ca77b9190fcd>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_max']=np.exp(-np.log(df['max_shortest_path_length']+1))\n","<ipython-input-11-ca77b9190fcd>:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampler_type_node_based']=df.apply(lambda row: is_node_based(row['sampling_algorithm']), axis=1)\n","<ipython-input-11-ca77b9190fcd>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampler_type_edge_based']=df.apply(lambda row: is_edge_based(row['sampling_algorithm']), axis=1)\n","<ipython-input-11-ca77b9190fcd>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampler_type_traversal_based']=df.apply(lambda row: is_traversal_based(row['sampling_algorithm']), axis=1)\n","<ipython-input-11-ca77b9190fcd>:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampling algorithm'] = df['sampling_algorithm']\n"]},{"output_type":"stream","name":"stdout","text":["Now: 14143, after: 14143\n","Target: HPD2_LCC\n","nalist  ['Unnamed: 0', 'entropy_clust_coeff', 'global_clust_coeff', 'global_clust_coeff_calc_time', 'node_betweenness_centrality_calc_time', 'min_shortest_path_length_LCC', 'max_shortest_path_length_LCC', 'var_shortest_path_length_LCC', 'mean_shortest_path_length_LCC', 'node_edge_betweenness_centrality_calc_time', 'min_edge_betweenness_centrality', 'max_edge_betweenness_centrality', 'mean_edge_betweenness_centrality', 'var_edge_betweenness_centrality', 'median_edge_betweenness_centrality', 'min_eccentricity_centrality', 'max_eccentricity_centrality', 'mean_eccentricity_centrality', 'median_eccentricity_centrality', 'var_eccentricity_centrality', 'entropy_degrees', 'diameter', 'diameter_calc_time', 'min_farness_centrality', 'max_farness_centrality', 'var_farness_centrality', 'mean_farness_centrality', 'median_farness_centrality', 'farness_centrality_calc_time', 'median_shortest_path_length_LCC', 'shortest_path_length_LCC_calc_time']\n","Empty DataFrame\n","Columns: [Unnamed: 0, graph_ID, min_clust_coeff, max_clust_coeff, mean_clust_coeff, var_clust_coeff, median_clust_coeff, clust_coeff_calc_time, min_node_betweenness_centrality, max_node_betweenness_centrality, mean_node_betweenness_centrality, var_node_betweenness_centrality, median_node_betweenness_centrality, num_connected_components, max_connected_components_size, min_connected_components_size, mean_connected_components_size, var_connected_components_size, median_connected_components_size, connected_components_calc_time, min_eigenvector_centrality, max_eigenvector_centrality, mean_eigenvector_centrality, var_eigenvector_centrality, median_eigenvector_centrality, eigenvector_centrality_calc_time, min_pagerank_centrality, max_pagerank_centrality, mean_pagerank_centrality, var_pagerank_centrality, median_pagerank_centrality, pagerank_centrality_calc_time, min_degrees_max_spanning_tree, max_degrees_max_spanning_tree, mean_degrees_max_spanning_tree, var_degrees_max_spanning_tree, median_degrees_max_spanning_tree, max_spanning_tree_calc_time, min_shortest_path_length, max_shortest_path_length, var_shortest_path_length, mean_shortest_path_length, degree_assortativity, degree_assortativity_calc_time, node_nums, edge_nums, min_degree, max_degree, mean_degree, var_degree, median_degree, graph_density, node_nums_div_edge_nums, edge_nums_div_node_nums, sampling_percent, sampling_algorithm, run_time, run_time var, D3, C2D2, HPD2, HPD2_LCC, KS Degree Distr var, KS Clustering Coefficient Distr var, KS hop plots Distr var, KS hop plots LCC Distr var, partition, synthetic, entropy_clust_coeff, global_clust_coeff, global_clust_coeff_calc_time, node_betweenness_centrality_calc_time, min_shortest_path_length_LCC, max_shortest_path_length_LCC, var_shortest_path_length_LCC, mean_shortest_path_length_LCC, node_edge_betweenness_centrality_calc_time, min_edge_betweenness_centrality, max_edge_betweenness_centrality, mean_edge_betweenness_centrality, var_edge_betweenness_centrality, median_edge_betweenness_centrality, min_eccentricity_centrality, max_eccentricity_centrality, mean_eccentricity_centrality, median_eccentricity_centrality, var_eccentricity_centrality, entropy_degrees, diameter, diameter_calc_time, min_farness_centrality, max_farness_centrality, var_farness_centrality, mean_farness_centrality, median_farness_centrality, farness_centrality_calc_time, median_shortest_path_length_LCC, shortest_path_length_LCC_calc_time]\n","Index: []\n","\n","[0 rows x 98 columns]\n","Now: 14143, after: 14143\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-11-ca77b9190fcd>:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['eigenvector_centrality_max'] = np.exp(-np.log(df['max_eigenvector_centrality']+1))\n","<ipython-input-11-ca77b9190fcd>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_min']=df['min_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_avg']=df['mean_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:56: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_var']=df['var_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_median']=df['median_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_max']=np.exp(-np.log(df['max_degrees_max_spanning_tree']+1))\n","<ipython-input-11-ca77b9190fcd>:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_min'] = df['min_pagerank_centrality']/df['max_pagerank_centrality']\n","<ipython-input-11-ca77b9190fcd>:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_avg'] = df['mean_pagerank_centrality']/df['max_pagerank_centrality']\n","<ipython-input-11-ca77b9190fcd>:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_median'] = df['median_pagerank_centrality']/df['max_pagerank_centrality']\n","<ipython-input-11-ca77b9190fcd>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_var'] = np.exp(-np.log(df['var_pagerank_centrality']+1))\n","<ipython-input-11-ca77b9190fcd>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_max']=np.exp(-np.log(df['max_pagerank_centrality']+1))\n","<ipython-input-11-ca77b9190fcd>:73: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_min'] = df['min_shortest_path_length']/df['max_shortest_path_length']\n","<ipython-input-11-ca77b9190fcd>:74: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_avg'] = df['mean_shortest_path_length']/df['max_shortest_path_length']\n","<ipython-input-11-ca77b9190fcd>:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_var'] = np.exp(-np.log(df['var_shortest_path_length']+1))\n","<ipython-input-11-ca77b9190fcd>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_max']=np.exp(-np.log(df['max_shortest_path_length']+1))\n","<ipython-input-11-ca77b9190fcd>:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampler_type_node_based']=df.apply(lambda row: is_node_based(row['sampling_algorithm']), axis=1)\n","<ipython-input-11-ca77b9190fcd>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampler_type_edge_based']=df.apply(lambda row: is_edge_based(row['sampling_algorithm']), axis=1)\n","<ipython-input-11-ca77b9190fcd>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampler_type_traversal_based']=df.apply(lambda row: is_traversal_based(row['sampling_algorithm']), axis=1)\n","<ipython-input-11-ca77b9190fcd>:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampling algorithm'] = df['sampling_algorithm']\n"]},{"output_type":"stream","name":"stdout","text":["Now: 14143, after: 14143\n","Target: run_time\n","nalist  ['Unnamed: 0', 'entropy_clust_coeff', 'global_clust_coeff', 'global_clust_coeff_calc_time', 'node_betweenness_centrality_calc_time', 'min_shortest_path_length_LCC', 'max_shortest_path_length_LCC', 'var_shortest_path_length_LCC', 'mean_shortest_path_length_LCC', 'node_edge_betweenness_centrality_calc_time', 'min_edge_betweenness_centrality', 'max_edge_betweenness_centrality', 'mean_edge_betweenness_centrality', 'var_edge_betweenness_centrality', 'median_edge_betweenness_centrality', 'min_eccentricity_centrality', 'max_eccentricity_centrality', 'mean_eccentricity_centrality', 'median_eccentricity_centrality', 'var_eccentricity_centrality', 'entropy_degrees', 'diameter', 'diameter_calc_time', 'min_farness_centrality', 'max_farness_centrality', 'var_farness_centrality', 'mean_farness_centrality', 'median_farness_centrality', 'farness_centrality_calc_time', 'median_shortest_path_length_LCC', 'shortest_path_length_LCC_calc_time']\n","Empty DataFrame\n","Columns: [Unnamed: 0, graph_ID, min_clust_coeff, max_clust_coeff, mean_clust_coeff, var_clust_coeff, median_clust_coeff, clust_coeff_calc_time, min_node_betweenness_centrality, max_node_betweenness_centrality, mean_node_betweenness_centrality, var_node_betweenness_centrality, median_node_betweenness_centrality, num_connected_components, max_connected_components_size, min_connected_components_size, mean_connected_components_size, var_connected_components_size, median_connected_components_size, connected_components_calc_time, min_eigenvector_centrality, max_eigenvector_centrality, mean_eigenvector_centrality, var_eigenvector_centrality, median_eigenvector_centrality, eigenvector_centrality_calc_time, min_pagerank_centrality, max_pagerank_centrality, mean_pagerank_centrality, var_pagerank_centrality, median_pagerank_centrality, pagerank_centrality_calc_time, min_degrees_max_spanning_tree, max_degrees_max_spanning_tree, mean_degrees_max_spanning_tree, var_degrees_max_spanning_tree, median_degrees_max_spanning_tree, max_spanning_tree_calc_time, min_shortest_path_length, max_shortest_path_length, var_shortest_path_length, mean_shortest_path_length, degree_assortativity, degree_assortativity_calc_time, node_nums, edge_nums, min_degree, max_degree, mean_degree, var_degree, median_degree, graph_density, node_nums_div_edge_nums, edge_nums_div_node_nums, sampling_percent, sampling_algorithm, run_time, run_time var, D3, C2D2, HPD2, HPD2_LCC, KS Degree Distr var, KS Clustering Coefficient Distr var, KS hop plots Distr var, KS hop plots LCC Distr var, partition, synthetic, entropy_clust_coeff, global_clust_coeff, global_clust_coeff_calc_time, node_betweenness_centrality_calc_time, min_shortest_path_length_LCC, max_shortest_path_length_LCC, var_shortest_path_length_LCC, mean_shortest_path_length_LCC, node_edge_betweenness_centrality_calc_time, min_edge_betweenness_centrality, max_edge_betweenness_centrality, mean_edge_betweenness_centrality, var_edge_betweenness_centrality, median_edge_betweenness_centrality, min_eccentricity_centrality, max_eccentricity_centrality, mean_eccentricity_centrality, median_eccentricity_centrality, var_eccentricity_centrality, entropy_degrees, diameter, diameter_calc_time, min_farness_centrality, max_farness_centrality, var_farness_centrality, mean_farness_centrality, median_farness_centrality, farness_centrality_calc_time, median_shortest_path_length_LCC, shortest_path_length_LCC_calc_time]\n","Index: []\n","\n","[0 rows x 98 columns]\n","Now: 14143, after: 14143\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-11-ca77b9190fcd>:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['eigenvector_centrality_max'] = np.exp(-np.log(df['max_eigenvector_centrality']+1))\n","<ipython-input-11-ca77b9190fcd>:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_min']=df['min_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_avg']=df['mean_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:56: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_var']=df['var_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_median']=df['median_degrees_max_spanning_tree']/df['max_degrees_max_spanning_tree']\n","<ipython-input-11-ca77b9190fcd>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['degrees_spanning_tree_max']=np.exp(-np.log(df['max_degrees_max_spanning_tree']+1))\n","<ipython-input-11-ca77b9190fcd>:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_min'] = df['min_pagerank_centrality']/df['max_pagerank_centrality']\n","<ipython-input-11-ca77b9190fcd>:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_avg'] = df['mean_pagerank_centrality']/df['max_pagerank_centrality']\n","<ipython-input-11-ca77b9190fcd>:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_median'] = df['median_pagerank_centrality']/df['max_pagerank_centrality']\n","<ipython-input-11-ca77b9190fcd>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_var'] = np.exp(-np.log(df['var_pagerank_centrality']+1))\n","<ipython-input-11-ca77b9190fcd>:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['pagerank_centrality_max']=np.exp(-np.log(df['max_pagerank_centrality']+1))\n","<ipython-input-11-ca77b9190fcd>:73: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_min'] = df['min_shortest_path_length']/df['max_shortest_path_length']\n","<ipython-input-11-ca77b9190fcd>:74: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_avg'] = df['mean_shortest_path_length']/df['max_shortest_path_length']\n","<ipython-input-11-ca77b9190fcd>:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_var'] = np.exp(-np.log(df['var_shortest_path_length']+1))\n","<ipython-input-11-ca77b9190fcd>:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['shortest_path_length_max']=np.exp(-np.log(df['max_shortest_path_length']+1))\n","<ipython-input-11-ca77b9190fcd>:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampler_type_node_based']=df.apply(lambda row: is_node_based(row['sampling_algorithm']), axis=1)\n","<ipython-input-11-ca77b9190fcd>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampler_type_edge_based']=df.apply(lambda row: is_edge_based(row['sampling_algorithm']), axis=1)\n","<ipython-input-11-ca77b9190fcd>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampler_type_traversal_based']=df.apply(lambda row: is_traversal_based(row['sampling_algorithm']), axis=1)\n","<ipython-input-11-ca77b9190fcd>:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df['sampling algorithm'] = df['sampling_algorithm']\n"]},{"output_type":"stream","name":"stdout","text":["Now: 14143, after: 14143\n"]}],"source":["import pandas as pd\n","import numpy as np\n","model_num = '2'\n","\n","for target in ['D3', 'C2D2', 'HPD2', 'HPD2_LCC', 'run_time']:\n","    dataset = generate_dataset(sources, target)\n","\n","    # data quality check\n","    print('Target: {}'.format(target))\n","    data_quality_check(dataset)\n","    now_vs_after(dataset)\n","\n","    dfx = generate_features(dataset).drop_duplicates().fillna(0)\n","    now_vs_after(dfx)\n","    all_algorithms = ['forest fire', 'random degree node', 'random edge', 'random jump', 'random node', 'random node edge', 'snowball', 'frontier', 'rank degree', 'induced random edge', 'metropolis hastings random walk', 'expansion']\n","    for alg in all_algorithms:\n","        if not alg in dfx.columns: # alg not in df\n","            dfx[alg] = 0\n","    dfx = dfx.rename(columns={\"KS Degree Distr\": \"D3\", \"KS Clustering Coefficient Distr\": \"C2D2\", 'KS hop plots Distr': 'HPD2', 'KS hop plots LCC Distr': 'HPD2_LCC'})\n","    dfx.to_csv(root_folder + 'data/model_' + model_num + '/{}_v4.csv'.format(target), index=False)"]},{"cell_type":"code","execution_count":null,"id":"8f3e7027-93b7-488f-91a1-7b571bf26958","metadata":{"id":"8f3e7027-93b7-488f-91a1-7b571bf26958","outputId":"8938367c-8c2d-4929-aeb3-f2bb240bf948"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>clust_coeff_calc_time</th>\n","      <th>connected_components_calc_time</th>\n","      <th>pagerank_centrality_calc_time</th>\n","      <th>max_spanning_tree_calc_time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.155451</td>\n","      <td>0.440589</td>\n","      <td>0.454012</td>\n","      <td>1.744806</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>2.067737</td>\n","      <td>0.920141</td>\n","      <td>0.321629</td>\n","      <td>9.598539</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.018222</td>\n","      <td>0.765105</td>\n","      <td>0.528590</td>\n","      <td>6.645228</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.130755</td>\n","      <td>0.269564</td>\n","      <td>0.256353</td>\n","      <td>4.912762</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.205690</td>\n","      <td>0.193303</td>\n","      <td>0.760978</td>\n","      <td>3.227988</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>475</th>\n","      <td>9.447464</td>\n","      <td>0.664761</td>\n","      <td>0.824831</td>\n","      <td>11.493466</td>\n","    </tr>\n","    <tr>\n","      <th>476</th>\n","      <td>7.822001</td>\n","      <td>3.362185</td>\n","      <td>0.783072</td>\n","      <td>17.071564</td>\n","    </tr>\n","    <tr>\n","      <th>477</th>\n","      <td>4.993967</td>\n","      <td>0.087790</td>\n","      <td>0.752546</td>\n","      <td>6.690350</td>\n","    </tr>\n","    <tr>\n","      <th>478</th>\n","      <td>5.315836</td>\n","      <td>0.321391</td>\n","      <td>0.650829</td>\n","      <td>11.097322</td>\n","    </tr>\n","    <tr>\n","      <th>479</th>\n","      <td>14.282801</td>\n","      <td>0.265360</td>\n","      <td>0.530702</td>\n","      <td>40.759410</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>487 rows × 4 columns</p>\n","</div>"],"text/plain":["     clust_coeff_calc_time  connected_components_calc_time  \\\n","0                 0.155451                        0.440589   \n","0                 2.067737                        0.920141   \n","1                 2.018222                        0.765105   \n","1                 1.130755                        0.269564   \n","2                 0.205690                        0.193303   \n","..                     ...                             ...   \n","475               9.447464                        0.664761   \n","476               7.822001                        3.362185   \n","477               4.993967                        0.087790   \n","478               5.315836                        0.321391   \n","479              14.282801                        0.265360   \n","\n","     pagerank_centrality_calc_time  max_spanning_tree_calc_time  \n","0                         0.454012                     1.744806  \n","0                         0.321629                     9.598539  \n","1                         0.528590                     6.645228  \n","1                         0.256353                     4.912762  \n","2                         0.760978                     3.227988  \n","..                             ...                          ...  \n","475                       0.824831                    11.493466  \n","476                       0.783072                    17.071564  \n","477                       0.752546                     6.690350  \n","478                       0.650829                    11.097322  \n","479                       0.530702                    40.759410  \n","\n","[487 rows x 4 columns]"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["dfx[['clust_coeff_calc_time',\n","       'connected_components_calc_time', 'pagerank_centrality_calc_time',\n","       'max_spanning_tree_calc_time']]"]},{"cell_type":"code","execution_count":null,"id":"2698dbd9-042a-417b-a379-990052391f18","metadata":{"id":"2698dbd9-042a-417b-a379-990052391f18"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}